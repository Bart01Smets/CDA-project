---
title: "Analysis of Continuous Data project"
author: "Thomas Sertijn, Bart Smets, Ilja Van Bever, Lieselot Van de Putte"
date: "2025-11-09"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      echo = TRUE)
```

# Protocol - Univariate part

```{r echo=FALSE}
library(knitr)
library (glue)
library(dplyr)
```
## Research question

For this research, we want to investigate how  socio-economic disadvantage relates to violent crime rates. More specifically we want to explore the association between poverty and violent crime rates in the USA. 
In his seminal work, Becker (1968) stated that the decision to commit crime is a rational choice where people weigh the benefits and costs against each other. It could then be argued that the incentive to commit crime is higher for people who have a lower income, as they ‘need’ the benefits more. Following this, we would then also expect that in communities with a higher poverty rate, there will also be higher crime rates. 
Depending on the results of our analysis, it could be used to inform the relevant policies. It would, for example, give another argument to redistributive policies: if an effect is found, this policy would also result in a reduction in violent crime, next to an economic benefit. Our analysis hopes to shed further light on this issue.

For the purpose of our research question we selected the following predictor variables:

- **PctPopUnderPov**: percentage of people under the poverty level (main predictor)
- **perCapInc**: per capita income. While similar to PctPopUnderPov, this takes the whole income distribution into account and not just the lower end. If this average is lower, then we expect more crime to happen.
- **PctEmploy**: We could argue that if more people are employed less people have an incentive to commit crime.
- **PctLess9thGrade**, **PctNotHSGrad**, or **PctBSorMore**: percentage of people 25 and over with less than a 9th grade education. Education leads to a higher socio-economic standing, which would suggest that people have less reason to commit crime. 
- **NummImmig**, **RacePctBlac**, and **AgePct12t29** are included because they are often linked to differences in socio-economic conditions, which may influence crime rates.

Next, we outline how we our analyses help to answer the outlined research questions.

1. What is the association between poverty rates (*PctPopUnderPov*) and violent crime rates (*ViolentCrimesPerPop*) in U.S. communities? We first present an univariate analysis to gain an initial understanding of whether our hypothesis holds, how strong the relationship is, but do not take any confounding factors into account.

2. How does the association between poverty and violent crime rates change when we do account for other socio-economic factors? Specifically, using a multivariate analysis, we examine whether the poverty-crime relationship is confounded by or interacts with other socio-economic variables. On top of this, it also allows us to assess how much of the variation in violent crime rates these other variables explain and thus makes it possible for our model to more accurately predict crime rates. This analysis allows us to better isolate the independent effect of poverty, while also gaining a better understanding of how this effect depends on interactions and which other variables are important. This is important, as it helps us establish whether poverty reduction alone would reduce crime rates or if interventions should also target other socio-economic factors.

## Design of the study
The dataset combines 1990 U.S. Census socio-economic data, 1990 law enforcement data from the Law Enforcement Management and Admin Stats (LEMAS) survey, and 1995 FBI crime data, thereby creating two cohorts. For the FBI crime data, it is mentioned that states with a lower amount of visitors have a lower per capita crime rate and vice verse. The LEMAS survey covers all communities with police departments of at least 100 officers and a random sample of smaller departments. If communities were absent from either the crime or census datasets (e.g., those with very small departments), then they were removed. All demographic data is from 1990, but per-capita crime rates use 1995 population counts. Finally, rape counts, a component of violent crime, are missing in some states due to inconsistent reporting,  which resulted in missing total violent crime values for those states. We will investigate whether this missingness is random and if necessary use imputations.

## Descriptive analysis
To get a first impression of the data, a univariate descriptive analysis will be performed for the response variable and candidate predictor variables (all continuous). The most common univariate statistics are calculated: the sample size, mean, standard deviation, minimum, first quartile, median, the third quartile and the maximum. We also describe the missing values and discuss whether this missingness is random or not and how it affects our conclusions.

The distributions of the variables are visualized by boxplots, QQ plots and histograms. Outliers are identified using Tukey’s 1.5 x IQR rule. We also investigate the population size, as it can influence the reliability of the data points: small communities may have a higher probability to have more extreme values of the predictor and response variables by the fact that the denominator in the response variable (total number of violent crimes per 100K population) is smaller. In the regression phase this will be used to investigate the outlier values. To find what the relationship is between the main predictor variable and the potential extra predictor variables, scatter plots with smoothers are made for the bivariate relationships and correlations are checked. Given that the data comes from two cohorts, we will investigate and account for its effects.

## Data split, linear regression and assumption checks
Before performing linear regression and building models, the dataset is randomly split into a training set (80% of the data) and a holdout set (20% of the data). This holdout set will be used to validate the final model. Using the training set, we then investigate the association between the main predictor variable and the response variable, a simple  linear regression is first fitted and the output is subsequently evaluated. The following statistics are calculated and discussed: regression coefficients, the F-statistics (/t-statistics), the R squared, the MSE, the p-value, and standard error of the slope. We also construct confidence intervals, and evaluate outliers. Subsequently, these outliers are further evaluated, e.g. are outliers linked to communities with a small population size. Ultimately, our regression equation is given as follows:

$$
ViolentCrimesP erP op_i = \beta_0 + \beta_1PctpopUnderPov_i + \epsilon_i 
$$
We then perform several checks to see whether assumptions, such as linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors, hold. Concretely, we do so by plotting residuals vs. fitted values for the linearity and independence of errors, squared residuals vs. fitted values for homoscedasticity checks and using QQ-plots of residuals to check for normality.

## Protocol - Multivariate regression
Socioeconomic disadvantage can be estimated from multiple variables in the study. Due to the large number of options and nature of our research question, it was decided that finding an optimal model using a limited number of predictors is beneficial to answer the question in mind. For this, we intend to use an All-possible regressions procedure on the 80% of the data labeled for training in order to select the most meaningful variables. We only include models with a maximum of 5 predictor variables, and only models with 0, 1 or 2 education predictor variables will be evaluated. In order to decide the best model, we intend to choose the best model under BIC. This measure penalizes for adding more variables to the model in a large sample. To test if the chosen variables are in the right format in the model, we can use partial regression plots to visualize the effect of the specific variables on the rest of the model. If needed transformations may be used. To choose which interaction terms to include, we add all interaction terms of the predictor variables with our main predictor variable to the selected multivariate model and evaluate their significance. We retain the interaction term(s) that are found to have a significant effect, including at least one.  

## Model fit and outliers
The assumptions regarding linearity, homoscedasticity, normality on residuals and independence on the model residuals will be checked, in the same way as described for the univariate model.
After specifying the multivariate regression model, several diagnostic measures will be used to assess model fit and identify influential outlying data points. First, we compute the studentized residuals to find outliers in the outcome variable. To determine how strongly each observation impacts model estimates, we determine the leverage, the Cook’s distance, DFBETAS and DFFITS. Outliers will be examined in context and only removed if clearly justified. If removal is not justified, influence might be dampened by employing Robust Regression. We will check for multicollinearity between variables by calculating the Variance Inflaction Factor (VIF) and if necessary adapt our analysis.

## Model Validation
Using the validation set, it is possible to validate our model. For this purpose, we make use of  the Mean Squared Prediction Error (MSPR). If the model is able to make good predictions, this MSPR would be close to the original Mean Squared Error (MSE) of the model.

## Interpretation of the parameters


```{r echo=FALSE}
# Create the table in R
schedule <- data.frame(
  Deadline = c("3/11", "10/11", "17/11", "24/11", "24/11", "1/12", "1/12", "1/12", "8/12", "8/12", "8/12"),
  Subject = c("Data extraction",
              "Descriptive analyses",
              "Model building",
              "Model interpretation",
              "Prediction with linear model",
              "Statistical discussion linear model",
              "Fitting GLM",
              "Fitting the final model",
              "Prediction with GLM",
              "Statistical discussion GLM",
              "Final conclusion and discussion"),
  Final_responsibility = c("Thomas",
                           "Ilja",
                           "Thomas",
                           "Lieselot",
                           "Ilja",
                           "Ilja",
                           "Lieselot",
                           "Thomas",
                           "Ilja",
                           "Thomas",
                           "Lieselot")
)

# Print the table
kable(schedule, caption = "Project Schedule Overview", align = c('c', 'l', 'c'))
```

# Data extraction:
---- load data ----
```{r}
library(data.table)
violent_crimes_table <- fread("curl https://archive.ics.uci.edu/static/public/211/communities+and+crime+unnormalized.zip")
```
---- remotely get variable names ---- 
```{r}
library(dplyr)
library(stringr)
library(rvest)
url <- "https://archive.ics.uci.edu/dataset/211/communities+and+crime+unnormalized"

# Read the HTML page
page <- read_html(url)

# Extract all text from the page
text <- page %>% html_text()

# Split into lines
lines <- str_split(text, "\n")[[1]]

start <- grep("Additional Variable Information", lines, ignore.case = TRUE)
end   <- grep("Summary Statistics:", lines, ignore.case = TRUE)

# only retain the lines starting with --
var_lines <- lines[str_starts(str_trim(lines), "--")]
# remove the --
var_names <- sapply(strsplit(var_lines, "--"), function(x) str_trim(x[2]))
# only retain the variable names by cutting everything after the :
var_names <- str_extract(var_names, "^[^:]+")
```

```{r}
colnames(violent_crimes_table) <- var_names 
```

```{r}
crimes_table_subset <- violent_crimes_table %>% 
  dplyr::select(communityname,state,countyCode, communityCode, fold, population, 
         PctPopUnderPov, perCapInc, PctEmploy, PctLess9thGrade, PctNotHSGrad, PctBSorMore,
         NumImmig, racepctblack, agePct12t29, ViolentCrimesPerPop
         )
```

# Design

The dataset combines 1990 U.S. Census socio-economic data, 1990 law enforcement data from the Law Enforcement Management and Admin Stats (LEMAS) survey, and 1995 FBI crime data, thereby creating two cohorts. For the FBI crime data, it is mentioned that states with a lower amount of visitors have a lower per capita crime rate and vice verse. The LEMAS survey covers all communities with police departments of at least 100 officers and a random sample of smaller departments. If communities were absent from either the crime or census datasets (e.g., those with very small departments), then they were removed. All demographic data is from 1990, but per-capita crime rates use 1995 population counts. Finally, rape counts, a component of violent crime, are missing in some states due to inconsistent reporting, which resulted in missing total violent crime values for those states. We will investigate whether this missingness has probably a large effect on the model and if necessary use imputations.

# Data preparation

```{r, include=FALSE}
library(ggplot2)
library(gtsummary)
library(sjlabelled)
library(tidyr)
```

Since the outcome variable *ViolentCrimesPerPop* (total number of violent crimes per 100K population) is expressed relative to the population size, the variable *NumImmig* is converted (by dividing it by the population size and multiplying by 100%). It's important to mention that this is not an exact transformation, because all demographic
data is from 1990, but per-capita crime rates use 1995 population counts. 
```{r}
crimes_table_subset$ViolentCrimesPerPop <- as.numeric(crimes_table_subset$ViolentCrimesPerPop)
crimes_table_subset$PctImmig <- crimes_table_subset$NumImmig/crimes_table_subset$population*100
crimes_table_subset = crimes_table_subset[,-c('NumImmig', 'fold')]
```
```{r}
sjlabelled::set_label(crimes_table_subset) <- c("communityname", "state", "countyCode", "communityCode", "population", "people under the poverty level (%)", "per capita income ($)", "percentage of people 16 and over who are employed (%)", "percentage of people 25 and over with less than a 9th grade education (%)", "percentage of people 25
or over, that have not graduated highschool (%)", "percentage of people 25 or over, with
at least a bachelor’s degree (%)", "percentage of population that is african american (%)", "percentage of population that is 12-29 in age (%)", "total number of violent crimes per 100K population", "percentage of immigrants (%)")
```


It is examined how many NA values are present in the database.
```{r counting NA values}
crimes_table_subset %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  group_by(variable) %>%
  summarise(
    NAs = sum(is.na(value))
  )
```

For 221 of the 1994 observations NA values are found for the outcome variable *ViolentCrimesPerPop*. Imputations can not help a lot to handle this. As already mentioned in the section on the study design, many of these NA values are due to the fact that rape counts, a component of violent crime, were not included in the statistics for several states. The rows where the outcome variable has an NA value are removed, as these rows are not useful for the regression. It can be noted that the variables *countyCode* and *communityCode* are also frequently unknown.

```{r identifying NA values}
na_subset <- crimes_table_subset %>%
  filter(is.na(ViolentCrimesPerPop)
  )
na_subset <- na_subset[,-'ViolentCrimesPerPop']
na_subset
```


```{r}
crimes_table_subset = na.omit(crimes_table_subset)
colSums(crimes_table_subset == "?", na.rm = TRUE)
```

# Univariate descriptives

After removing NA values from the database univariate descriptives are calculated, both the missing values and the non-missing values. 

```{r summary statistics}
#str(crimes_table_subset)
#summary(crimes_table_subset)
crimes_table_subset %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  group_by(variable) %>%
  summarise(
    min = min(value, na.rm = TRUE),
    q25 = quantile(value, 0.25, na.rm = TRUE),
    mean = mean(value, na.rm = TRUE),
    sd = sd(value, na.rm = TRUE),
    q75 = quantile(value, 0.75, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    n = n(),
    NAs = sum(is.na(value))
  )
```
```{r summary statistics nas}
na_subset %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  group_by(variable) %>%
  summarise(
    min = min(value, na.rm = TRUE),
    q25 = quantile(value, 0.25, na.rm = TRUE),
    mean = mean(value, na.rm = TRUE),
    sd = sd(value, na.rm = TRUE),
    q75 = quantile(value, 0.75, na.rm = TRUE),
    max = max(value, na.rm = TRUE),
    n = n(),
    NAs = sum(is.na(value))
  )
```

To gain insight into the univariate distributions, boxplots and histograms are generated.

```{r boxplots, fig.width=8, fig.height=12}
numeric_cols_na <- sapply(na_subset, is.numeric)
crimes_table_subset_num_na <- na_subset[, ..numeric_cols_na]
crimes_table_subset$logpopulation <- log10(crimes_table_subset$population)
numeric_cols <- sapply(crimes_table_subset, is.numeric)
crimes_table_subset_num <- crimes_table_subset[, ..numeric_cols]
par(mfrow = c(4,2))
for(columnname in names(crimes_table_subset_num_na)){
  column <- crimes_table_subset_num[[columnname]]
  boxplot(column,
          main = columnname
          )
  hist(column,   
       main = columnname,
       xlab = get_label(column)
          )
}
```
```{r boxplots na comparison, fig.width=8, fig.height=12, eval=FALSE, include=FALSE}
par(mfrow = c(4,2))
for(columnname in names(crimes_table_subset_num_na)){
  column <- crimes_table_subset_num[[columnname]]
  column_na <- crimes_table_subset_num_na[[columnname]]
  boxplot(column,
          main = columnname
          )
  boxplot(column_na,
          main = paste(columnname, "(missing data)")
          )
}
```
To investigate the impact of the missing values, the distribution of the other variables is compared in the missing data vs. the non-missing data. This is done using the histograms. Neither the summary statistics nor the scatter plots indicate that the missing data have characteristics that differ substantially from the non-missing data
```{r histograms na comparison, fig.width=8, fig.height=12}
numeric_cols_na <- sapply(na_subset, is.numeric)
crimes_table_subset_num_na <- na_subset[, ..numeric_cols_na]
par(mfrow = c(4,2))
for(columnname in names(crimes_table_subset_num_na)){
  column <- crimes_table_subset_num[[columnname]]
  column_na <- crimes_table_subset_num_na[[columnname]]
  hist(column,   
       main = columnname,
       xlab = get_label(column)
          )
  hist(column_na,   
       main = paste(columnname, "(missing data)"),
       xlab = get_label(column)
          )
}
```

# Multivariate descriptives

After investigating the univariate descriptives, multivariate descriptives are calculated.

The correlation matrix shows the extent to which the variables in the dataset are correlated with each other.
Below, all variables are listed, sorted from highest to lowest correlation with the outcome variable.

- *racepctblack* ($r = 0.63$)
- *PctPopUnderPov* ($r = 0.51$)
- *PctNotHSGrad* ($r = 0.47$)
- *PctLess9thGrade* ($r = 0.37$)
- *PctEmploy* ($r = -0.32$)
- *perCapInc* ($r = -0.32$)
- *PctBSorMore* ($r = 0.3$)
- *PctImmig* ($r = 0.19$)
- *agePct12t29* ($r = 0.11$)

It's important to mention that these correlations are indicators of an association, not of a causation.

The following predictors are highly correlated with each other. Therefore, it is best not to include them together in a model later.

- *PctNotHSGrad* and *PctLess9thGrade* ($r = 0.93$)
- *perCapInc* and *PctBSorMore* ($r = 0.77$)
- *PctNotHSGrad* and *PctBSorMore* ($r = -0.75$)

However the choice for predictors for the model will be dealt with thoroughly during the model building.

It is noticeable that the variable *racepctblack* is the one most strongly correlated with the outcome variable ($r = 0.63$), even more than *PctPopUnderPov*, the head predictor that was chosen for this research.

It is noticable the the variables *PctImmig* and *PctPopUnderPov* have correlation coefficients that are really low. *PctImmig* would therefore not be a suitable parameter in a univariate model. Nevertheless, it will be included in the model building process. Especially in models with interaction effects, *PctImmig* may still be a useful predictor

```{r correlations}
cor_matrix <- cor(crimes_table_subset_num[,-c('population', 'logpopulation')])
cor_values <- as.data.frame(as.table(cor_matrix))

library(ggcorrplot)
ggcorrplot(cor_matrix, lab = TRUE, type = "lower", 
           lab_size = 3, colors = c("red", "white", "blue"))

```
The following scatter plots were generated:

- for each variable, a scatter plot showing the relationship with the outcome variable *ViolentCrimesPerPop*;
- for each variable, a scatter plot showing the relationship with the main predictor variable *PctPopUnderPov*.

The first series of scatter plots indicates that not all variables have a linear relationship with *ViolentCrimesPerPop*.
In particular, the following variables do not appear to exhibit a clear linear trend:

- *perCapInc*
- *agePct12t29* (which also had a very low correlation coefficient)

Other variables show a somewhat linear pattern, although this trend is often distorted in the extreme regions of the x-axis.

The second series of scatter plots suggests that some variables exhibit a linear relationship with the main predictor *PctPopUnderPov*.
In particular, the following variables appear to show a fairly linear trend:

- *PctEmploy*
- *PctLess9thGrade*
- *PctNotHSGrad*

This implies that these variables are probably not suitable as additional predictors when *PctPopUnderPov* is already included in the model as this may cause multicollinearity.

```{r scatter plots, fig.width=8, fig.height=12}
x_vars <- colnames(crimes_table_subset_num)
dict_labels <- setNames(sapply(x_vars, function(x_var) get_label(crimes_table_subset_num[[x_var]])), x_vars)

library(ggplot2)
library(patchwork)
df <- crimes_table_subset_num[,-c("population", "logpopulation")]
y_var <- "ViolentCrimesPerPop"
x_vars <- setdiff(colnames(df), y_var)
plots <- lapply(x_vars, function(x_var) {
    ggplot(df, aes_string(x_var,y_var)) +
    geom_point(alpha = 0.6, size = 0.7) +
    geom_smooth(method = "lm", color = "blue", se = FALSE)+
    geom_smooth(method = "loess", color = "red", se = FALSE)+
    theme_bw(base_size = 8) +
    labs(x = str_wrap(paste(x_var, " (", dict_labels[x_var], ")", sep = ""), width = 45))
}
)
# Print 9 plots per pg
print(wrap_plots(plots, ncol = 3))

df <- crimes_table_subset_num[,-c("population", "ViolentCrimesPerPop", "logpopulation")]
y_var <- "PctPopUnderPov"
x_vars <- setdiff(colnames(df), y_var)
plots <- lapply(x_vars, function(x_var) {
    ggplot(df, aes_string(x_var,y_var))+
    geom_point(alpha = 0.6, size = 0.7) +
    geom_smooth(method = "lm", color = "blue", se = FALSE)+
    geom_smooth(method = "loess", color = "red", se = FALSE)+
    theme_bw(base_size = 8) +
    labs(x = str_wrap(paste(x_var, " (", dict_labels[x_var], ")", sep = ""), width = 45))
}
)

# Print 9 plots per pg
print(wrap_plots(plots, ncol = 3))
```
The scatter plots reveal one outlier for the *ViolentCrimesPerPop* variable. This outlier is the community Chestercity. For now the datapoint is kept for further analysis. In the diagnostics section, we will further investigate which data points can be considered outliers and what their influence is on the model.

```{r outlier}
crimes_table_subset[order(crimes_table_subset_num$ViolentCrimesPerPop, decreasing=TRUE), , drop = FALSE]
```


In the protocol it's stated that small communities may have a higher probability to have more extreme values of the predictor and response variables. To investigate this scatter plots are created with log(*population*) as x variable. It is clear that communities with a very small population show a very large spread for all variables.

```{r scatter plots population, fig.width=8, fig.height=12}
df <- crimes_table_subset_num
x_var <- "logpopulation"
y_vars <- setdiff(colnames(df), x_var)
plots <- lapply(y_vars, function(y_var) {
    ggplot(df, aes_string(x_var,y_var))+
    geom_point(alpha = 0.6, size = 0.7) +
    geom_smooth(method = "lm", color = "blue", se = FALSE)+
    geom_smooth(method = "loess", color = "red", se = FALSE)+
    theme_bw(base_size = 8) +
    labs(x = "log(population)", y = str_wrap(y_var, width = 45))
}
)

# Print 9 plots per pg
print(wrap_plots(plots, ncol = 3))

```

# Model Building

Before performing linear regression and building models, the dataset is randomly split into a training set (80% of the data) and a holdout set (20% of the data). This holdout set will be used to validate the final model.

```{r dataset split}
set.seed(987654321)
n <- nrow(crimes_table_subset_num)
training <- sample(1:n, size = floor(0.8 * n))
train_data <- crimes_table_subset[training, ]
test_data <- crimes_table_subset[-training, ]
n_training <- nrow(train_data)

cat("Training set size:", n_training, "\n")
cat("Test set size:", nrow(test_data), "\n")
```

## Univariate linear regression

 The simple univariate regression equation we estimate with the training set is given as follows:

$$
ViolentCrimesPerPop_i = \beta_0 + \beta_1 \cdot PctPopUnderPov_i + \epsilon_i
$$

```{r Univariate regression}
fit_simple <- lm(ViolentCrimesPerPop ~ PctPopUnderPov, data = train_data)
summary(fit_simple)
```
We show the relevant statistics to be discussed in this section:
```{r Extract relevant statistics}

cat("Regression equation: ViolentCrimesPerPop =", 
    round(coef(fit_simple)[1], 2), "+", 
    round(coef(fit_simple)[2], 2), "* PctPopUnderPov\n\n")

# R-squared and BIC-value
cat("R-squared:", round(summary(fit_simple)$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary(fit_simple)$adj.r.squared, 4), "\n")
cat("BIC:", BIC(fit_simple), "\n")

# MSE
sse_simple <- sum(fit_simple$residuals^2) 
mse_simple <- sse_simple/(n_training - 2) 
cat("SSE:", round(sse_simple, 2), "\n")
# Calculation of BIC-waarde in R: -2 * as.numeric(logLik(fit)) + attr(logLik(fit), "df") * log(n)
# Not the one from the course slides n*log(sse_simple) - n*log(n) + p*log(n), but ok?
cat("MSE:", round(mse_simple, 2), "\n")

# Confidence intervals for coefficients
cat("\n95% Confidence Intervals:\n")
print(confint(fit_simple))

# ANOVA
anova(fit_simple)
```

*PctPopUnderPov* = <mean> increase in violent crimes per 100K population if poverty rate increases by one percentage point

### Assumption checks

We check assumptions linearity, independence of errors, homoscedasticity, and normality of errors. It is clear these assumptions are violated. Larger outcome values tend to be underestimated, the variance for larger outcome values is larger and the QQ-plot shows violation of the normality assumption. In the next step, we extend the model by adding relevant predictors and reassess the assumptions.

```{r simple-lm-diagnostics, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))

#Residuals vs Fitted 
plot(fit_simple$fitted.values, fit_simple$residuals,
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red", lty = 2)
lines(lowess(fit_simple$fitted.values, fit_simple$residuals), col = "blue")

# Squared residuals vs Fitted
plot(fit_simple$fitted.values, fit_simple$residuals^2,
     xlab = "Fitted values", ylab = "Squared Residuals",
     main = "Squared Residuals vs Fitted")
lines(lowess(fit_simple$fitted.values, fit_simple$residuals^2), col = "blue")

# QQ-plot of residuals (normality)
qqnorm(fit_simple$residuals, main = "Normal Q-Q Plot of Residuals")
qqline(fit_simple$residuals, col = "red")
 
# Studentized residuals
stud_res <- rstudent(fit_simple)
plot(fit_simple$fitted.values, stud_res,
     xlab = "Fitted values", ylab = "Studentized Residuals",
     main = "Studentized Residuals vs Fitted")
outliers_simple <- which(abs(stud_res) > 2)
```

Figures to get a visual illustration of whether outliers are more common in small pop. We see larger residuals for larger populations. This means that the model tends te underestimate the total number of crimes for large populations.
```{r outliers vs population1}
par(mfrow = c(1, 2))
# Residuals vs population
plot(train_data$logpopulation, fit_simple$residuals,
     xlab = "log(population)", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
lines(lowess(train_data$logpopulation, fit_simple$residuals), col = "blue")

plot(train_data$logpopulation, stud_res,
     xlab = "log(population)", ylab = "Studentized Residuals")
abline(h = 0, col = "red", lty = 2)
lines(lowess(train_data$logpopulation, stud_res), col = "blue")
par(mfrow = c(1, 1))
```

## Model selection

We use an all-possible regressions procedure to select predictor variables. We include models with a maximum of 5 predictor variables and only models with 0, 1, or 2 education predictor variables and without interactions. The best model is chosen based on the Bayesian Information Criterion.

```{r model selection, message=FALSE}

max_extra_predictors <- 4
# Define predictor variables for model selection
predictors <- c("perCapInc", "PctEmploy", 
                "PctLess9thGrade", "PctNotHSGrad", "PctBSorMore",
                "racepctblack", "agePct12t29", "PctImmig")

# Educ variables
educ <- c("PctLess9thGrade", "PctNotHSGrad", "PctBSorMore")

formulas <- list()
for (i in 1:max_extra_predictors) {
  tmp <- combn(predictors, i)
  tmp <- apply(tmp, 2, paste, collapse=" + ")
  tmp <- paste0("ViolentCrimesPerPop~PctPopUnderPov + ", tmp)
  formulas[[i]] <- tmp
}
formulas <- unlist(formulas)
formulas <- sapply(formulas, as.formula)
models <- lapply(formulas, lm, data=train_data)

bics <- sapply(models, BIC)
r_square <- sapply(models, function(m) summary(m)$r.squared)
adj_r_square <- sapply(models, function(m) summary(m)$adj.r.squared)
formula_vector <- vapply(formulas, function(f) paste(deparse(f), collapse = "")
                         , character(1))

# build the frame
model_ranking <- data.frame(
  formula = formula_vector,
  r.square = r_square,
  adj.r.square = adj_r_square,
  BIC = bics
)
model_ranking <- model_ranking[order(model_ranking$BIC), ]
```

The best model according to the procedure is:
$$
ViolentCrimesPerPop_i = \beta_0 + \beta_1 \cdot PctPopUnderPov_i + \beta_2 \cdot PctLess9thGrade_i + \beta_3 \cdot PctNotHSGrad_i + \beta_4 \cdot PctImmig_i + \epsilon_i
$$

```{r show best model}
# Print best model
best <- which.min(model_ranking$BIC)
best_pred <- model_ranking$formula[best]
cat("\nBest model:\n")
cat(best_pred, "\n")
cat("BIC:", round(model_ranking$BIC[best], 2), "\n")
cat("Adjusted R²:", round(model_ranking$adj.r.square[best], 4), "\n")

fit_multi <- lm(best_pred, data = train_data)
multi_var_summary <- summary(fit_multi)
multi_var_summary
```

### Partial Regression Plots
The partial regression plots show that, with respect to the effects of the predictor variables, no clear deviation from linearity is visible. There is no transformation of the predictor variables recommended.

```{r partial-regression-plots, fig.width=10, fig.height=8}
predictor_list <- row.names(multi_var_summary$coefficients[-1,])

partial_regression_plot <- function(data, outcome, predictor, predictor_list) {
  temp <- data
  controls <- setdiff(predictor_list, predictor)
  
  formula_outcome  <- as.formula(paste(outcome, "~", 
                                       paste(controls, collapse = " + ")))
  formula_pred <- as.formula(paste(predictor, "~",
                                   paste(controls, collapse = " + ")))
  
  lm_outcome <- lm(formula_outcome, data = temp)
  temp$resid_outcome <- resid(lm_outcome)
  lm_pred <- lm(formula_pred, data = temp)
  temp$resid_pred <- resid(lm_pred)

ggplot(temp, aes(x = resid_pred, y = resid_outcome)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = paste("Residuals ", predictor, " ~ controls", sep = ""),
       y = paste("Residuals ", outcome, " ~ controls", sep = ""))
}

plots <- lapply(predictor_list, function(predictor) {
    partial_regression_plot(train_data, "ViolentCrimesPerPop", predictor, predictor_list)
}
)

# Print 9 plots per pg
print(wrap_plots(plots, ncol = 3))

```

### Assumption checks selected model
We see that the assumptions of normality and equal error variances of the error terms are again violated in the original multivariate model. 
```{r final model-diagnostics, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))

#Residuals vs Fitted
plot(fit_multi$fitted.values, fit_multi$residuals,
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red", lty = 2)
lines(lowess(fit_multi$fitted.values, fit_multi$residuals), col = "blue")

# Squared residuals vs Fitted
plot(fit_multi$fitted.values, fit_multi$residuals^2,
     xlab = "Fitted values", ylab = "Squared Residuals",
     main = "Squared Residuals vs Fitted")
lines(lowess(fit_multi$fitted.values, fit_multi$residuals^2), col = "blue")

# QQ-plot
qqnorm(fit_multi$residuals, main = "Normal Q-Q Plot")
qqline(fit_multi$residuals, col = "red")

# Studentized residuals
stud_res_multi <- rstudent(fit_multi)
plot(fit_multi$fitted.values, stud_res_multi,
     xlab = "Fitted values", ylab = "Studentized Residuals",
     main = "Studentized Residuals vs Fitted")
abline(h = 0, col = "red", lty = 2)
lines(lowess(fit_multi$fitted.values, stud_res_multi), col = "blue")

par(mfrow = c(1, 1))
```
A possible solution is to apply a log transformation of Y. The result shows approximate constant variances and residuals reasonably close to normal, applying the log transformation therefore offers a substantial improvement of our model. The reduction in R-squared suggests that the model-fit was inflated due to heteroscedasticity.  We still see slightly heavy tails. 

```{r}
fit_log <- lm(log(ViolentCrimesPerPop + 1) ~ 
                PctPopUnderPov + PctBSorMore + racepctblack + 
                PctImmig + PctPopUnderPov:PctBSorMore,
              data = train_data)

multi_var_summary <- summary(fit_log)
multi_var_summary
plot(fit_log)
par(mfrow = c(1, 1))
```

## Rerun selection after transformation of Y
We will repeat the model building process with the log-transformed ViolentCrimesPerPop.

```{r adding column with log transform}
train_data$logViolent <- log(train_data$ViolentCrimesPerPop + 1)
test_data$logViolent <- log(test_data$ViolentCrimesPerPop + 1)
```

## Model selection

We use an all-possible regressions procedure to select predictor variables. We include models with a maximum of 5 predictor variables and only models with 0, 1, or 2 education predictor variables. The best model is chosen based on the Bayesian Information Criterion.

```{r creating_formulas_without_interaction, message=FALSE}
max_extra_predictors <- 4
# Define predictor variables for model selection
predictors <- c("perCapInc", "PctEmploy", 
                "PctLess9thGrade", "PctNotHSGrad", "PctBSorMore",
                "racepctblack", "agePct12t29", "PctImmig")

# Educ variables
educ <- c("PctLess9thGrade", "PctNotHSGrad", "PctBSorMore")

formulas <- list()
for (i in 1:max_extra_predictors) {
  tmp <- combn(predictors, i)
  tmp <- apply(tmp, 2, paste, collapse=" + ")
  tmp <- paste0("logViolent~PctPopUnderPov + ", tmp)
  formulas[[i]] <- tmp
}
```


```{r model selection2, message=FALSE}
formulas <- unlist(formulas)
formulas <- sapply(formulas, as.formula)
models <- lapply(formulas, lm, data=train_data)

bics <- sapply(models, BIC)
r_square <- sapply(models, function(m) summary(m)$r.squared)
adj_r_square <- sapply(models, function(m) summary(m)$adj.r.squared)
formula_vector <- vapply(formulas, function(f) paste(deparse(f), collapse = "")
                         , character(1))

# build the frame
model_ranking <- data.frame(
  formula = formula_vector,
  r.square = r_square,
  adj.r.square = adj_r_square,
  BIC = bics
)
model_ranking <- model_ranking[order(model_ranking$BIC), ]
```

We then run the multivariate regression equation

```{r show best model 2}
# Print best model
best <- which.min(model_ranking$BIC)
best_pred <- model_ranking$formula[best]
cat("\nBest model:\n")
cat(best_pred, "\n")
cat("BIC:", round(model_ranking$BIC[best], 2), "\n")
cat("Adjusted R²:", round(model_ranking$adj.r.square[best], 4), "\n")

fit_multi <- lm(best_pred, data = train_data)
multi_var_summary <- summary(fit_multi)
multi_var_summary
```

### Partial Regression Plots

When creating the partial regression plots on the model with the log transformed violent crimes, no clear need for tranformations could be observed for all variables except PctRaceBlack. 

```{r partial-regression-plots2, fig.width=10, fig.height=8}
predictor_list <- row.names(multi_var_summary$coefficients[-1,])
plots <- lapply(predictor_list, function(predictor) {
    partial_regression_plot(train_data, "logViolent", predictor, predictor_list)
}
)

# Print 9 plots per pg
print(wrap_plots(plots, ncol = 3))
```

In the partial regression plot of the racepctblack, it could be seen that there is not a linear effect on the outcome. Therefore, multiple transformations on the racepctblack variable were tested, such as a log transformation. Eventually, it was decided that a logit transformation would work better given the percentile nature of the predictor.

```{r making logit tranformation}

pct_to_logit <-  function(percentage, addition = 1e-6) {
  percentiles_in_range01 <- percentage / 100
  # add small value to avoid 0 and 1
  percentiles_in_range01 <- pmin(
    pmax(percentiles_in_range01, addition),
    1 - addition)
  logit <- log(percentiles_in_range01 / 
                                     (1 - percentiles_in_range01))
  logit
}

train_data$race_black_logit <- pct_to_logit(train_data$racepctblack)
test_data$race_black_logit <- pct_to_logit(test_data$racepctblack)
```

Fitting the model with this logit transformed racepctblack seemed to stabilize the residual vs fitted plot, as well as make the partial regression plot look linear.

```{r}
fit_logit <- lm(logViolent ~ 
                PctPopUnderPov + 
                PctLess9thGrade +
                PctBSorMore + 
                race_black_logit +
                PctImmig,
              data = train_data)


multi_var_logit_summary <- summary(fit_logit)
multi_var_logit_summary
plot(fit_logit)
par(mfrow = c(1, 1))
```

```{r partial-regression-logits, fig.width=10, fig.height=8}
predictor_list <- row.names(multi_var_logit_summary$coefficients[-1,])
plots <- lapply(predictor_list, function(predictor) {
    partial_regression_plot(train_data, "logViolent", predictor, predictor_list)
}
)

# Print 9 plots per pg
print(wrap_plots(plots, ncol = 3))
```


### Interaction Terms Selection

Following the protocol, we add all interaction terms of the predictor variables with our main predictor variable *PctPopUnderPov* and evaluate their significance.

```{r interaction term selection 2}
selected_vars <- setdiff(
  row.names(multi_var_logit_summary$coefficients),
  c("(Intercept)", "PctPopUnderPov")
)

interaction_terms <- paste("PctPopUnderPov", selected_vars, sep = ":")

# Nieuwe code
formulas_interaction <- sapply(interaction_terms, function(i) {
  paste("logViolent ~", paste(predictor_list, collapse = " + "), "+",
                                  i)
})
# Oude code
# formulas <- sapply(interaction_terms, function(i) {
#   paste0(best_pred, " + ", i)
# })

formulas_interaction <- lapply(formulas_interaction, as.formula)
models <- lapply(formulas_interaction, lm, data=train_data)

interaction_models_form <- vapply(formulas_interaction, function(f) 
  paste(deparse(f), collapse = ""), character(1))
summary_val_extraction <- function(x, item) {
  tmp <- summary(x)$coefficients
  tmp[nrow(tmp), item]
}
p_vals_interaction <- sapply(models, function(m) 
  summary_val_extraction(m, "Pr(>|t|)"))
t_vals_interaction <- sapply(models, function(m) 
  summary_val_extraction(m, "t value"))

bics <- sapply(models, BIC)
r_square <- sapply(models, function(m) summary(m)$r.squared)
adj_r_square <- sapply(models, function(m) summary(m)$adj.r.squared)
delta_adj_r_square <- sapply(models, function(m) 
  summary(m)$adj.r.squared - summary(fit_multi)$adj.r.squared)

interaction_table <- data.frame(
  p.vals = p_vals_interaction,
  t.vals = t_vals_interaction,
  r.square = r_square,
  adj.r.square = adj_r_square,
  delta.adj = delta_adj_r_square,
  BIC = bics
)
interaction_table <- interaction_table[order(interaction_table$p.vals), ]
interaction_table
  
best_interaction <- row.names(interaction_table[1,])
```

This first selection showed that a model with an interaction term of PctPopUnderPov and PctLess9thGrade would be the only significant interaction term.

```{r final model2}
# Estimate model with interaction
formula_final <- as.formula(paste("logViolent ~", 
                                  paste(predictor_list, collapse = " + "), "+",
                                  "PctPopUnderPov:PctLess9thGrade"))
fit_final <- lm(formula_final, data = train_data)
summary(fit_final)
```

### Multicollinearity Check

Before checking model assumptions, we first assess multicollinearity using the Variance Inflation Factor (VIF) and remove a variable if necessary. 

```{r}
library(car)
# practicum: car::vif(fit)
vif <- vif(fit_final)
print(vif)
cat("BIC: ", BIC(fit_final))
```

However, when we look at the colinearity of the model we can see that the less then 9thgrade variables show a high multicolinearity score. One reason for this could be the addition of multiple education variables. For this reason the modelselection was retried with only one education variable present. 
```{r filtering formulas for education}

filtered_formulas <- Filter(function(f) {
  variables <- all.vars(f)              
  number_education_variables <- sum(variables %in% educ)   
  number_education_variables <= 1                     
}, formulas)
#change formulas to contain the logit based race variable
filtered_formulas <- lapply(filtered_formulas, function(f) {
  f_str <- deparse(f)                                # formula -> character vector
  f_str <- paste(f_str, collapse = " ")              # collapse into one string
  f_str <- gsub("racepctblack", "race_black_logit", f_str)  # replace variable name
  as.formula(f_str)                                  # back to formula
})


```

```{r}

models <- lapply(filtered_formulas, lm, data=train_data)

bics <- sapply(models, BIC)
r_square <- sapply(models, function(m) summary(m)$r.squared)
adj_r_square <- sapply(models, function(m) summary(m)$adj.r.squared)
formula_vector <- vapply(filtered_formulas, function(f) paste(deparse(f), collapse = "")
                         , character(1))

# build the frame
model_ranking <- data.frame(
  formula = formula_vector,
  r.square = r_square,
  adj.r.square = adj_r_square,
  BIC = bics
)
model_ranking <- model_ranking[order(model_ranking$BIC), ]
# Print best model
best <- which.min(model_ranking$BIC)
best_pred <- model_ranking$formula[best]
cat("\nBest model:\n")
cat(best_pred, "\n")
cat("BIC:", round(model_ranking$BIC[best], 2), "\n")
cat("Adjusted R²:", round(model_ranking$adj.r.square[best], 4), "\n")

fit_multi <- lm(best_pred, data = train_data)
multi_var_summary <- summary(fit_multi)
multi_var_summary
```

```{r}
predictor_list <- row.names(multi_var_summary$coefficients[-1,])
selected_vars <- setdiff(
  row.names(multi_var_summary$coefficients),
  c("(Intercept)", "PctPopUnderPov")
)

interaction_terms <- paste("PctPopUnderPov", selected_vars, sep = ":")

# Nieuwe code
formulas_interaction <- sapply(interaction_terms, function(i) {
  paste("logViolent ~", paste(predictor_list, collapse = " + "), "+",
                                  i)
})
# Oude code
# formulas <- sapply(interaction_terms, function(i) {
#   paste0(best_pred, " + ", i)
# })

formulas_interaction <- lapply(formulas_interaction, as.formula)
models <- lapply(formulas_interaction, lm, data=train_data)

interaction_models_form <- vapply(formulas_interaction, function(f) 
  paste(deparse(f), collapse = ""), character(1))
summary_val_extraction <- function(x, item) {
  tmp <- summary(x)$coefficients
  tmp[nrow(tmp), item]
}
p_vals_interaction <- sapply(models, function(m) 
  summary_val_extraction(m, "Pr(>|t|)"))
t_vals_interaction <- sapply(models, function(m) 
  summary_val_extraction(m, "t value"))

bics <- sapply(models, BIC)
r_square <- sapply(models, function(m) summary(m)$r.squared)
adj_r_square <- sapply(models, function(m) summary(m)$adj.r.squared)
delta_adj_r_square <- sapply(models, function(m) 
  summary(m)$adj.r.squared - summary(fit_multi)$adj.r.squared)

interaction_table <- data.frame(
  p.vals = p_vals_interaction,
  t.vals = t_vals_interaction,
  r.square = r_square,
  adj.r.square = adj_r_square,
  delta.adj = delta_adj_r_square,
  BIC = bics
)
interaction_table <- interaction_table[order(interaction_table$p.vals), ]
interaction_table
  
best_interaction <- row.names(interaction_table[1,])
```

```{r}
formula_final <- as.formula(paste("logViolent ~", 
                                  paste(predictor_list, collapse = " + "),  "+",
                                  best_interaction))
fit_final <- lm(formula_final, data = train_data)
summary(fit_final)
plot(fit_final)
par(mfrow = c(1, 1))
```

### Multicollinearity Check of model with one education variable:

```{r viskys}
vif <- vif(fit_final)
print(vif)
cat("BIC: ", BIC(fit_final))
```
When looking at the VIF values for this new model with the best interaction term with only one education variable, it could be seen that the pct population under poverty variable and its interaction with the age variable had a high VIF values, showing high multicolinearity. For this reason, the second best interaction term was tested in the model. 

```{r}
formula_final <- as.formula(paste("logViolent ~", 
                                  paste(predictor_list, collapse = " + "),  "+",
                                  "PctPopUnderPov:PctBSorMore"))
fit_final <- lm(formula_final, data = train_data)
final_sums <- summary(fit_final)
final_sums
plot(fit_final)
par(mfrow = c(1, 1))
```

```{r vivs2, ref.label="viskys"}
vif <- vif(fit_final)
print(vif)
cat("BIC: ", BIC(fit_final))
```
After checking for colinearity of this model, it could be seen to have a major improvement compared to the model with the PctPopUnderPov:agePct12t29. 

### partial regression plots of model with one education variable

To check if the variables are in the right form, another partial regression test is done. 

```{r fig.width=10, fig.height=8, warnings=FALSE}
predictor_list <- row.names(multi_var_summary$coefficients[-1,])
plots <- lapply(predictor_list, function(predictor) {
    partial_regression_plot(train_data, "logViolent", predictor, predictor_list)
}
)

# Print 9 plots per pg
print(wrap_plots(plots, ncol = 3))
```

```{r compare2}
# Compare models
cat("Comparison of models:\n")
cat("Original model adjusted R²: ", round(summary(fit_final)$adj.r.squared, 4), "\n") ## dit moet nog aangepast worden
cat("interaction term model adjusted R²:  ", round(summary(fit_final)$adj.r.squared, 4), "\n")

```

### Assumption checks final model

```{r final-model-diagnostics, fig.width=10, fig.height=8}
par(mfrow = c(2, 2))

#Residuals vs Fitted
plot(fit_final$fitted.values, fit_final$residuals,
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted (Final Model)")
abline(h = 0, col = "red", lty = 2)
lines(lowess(fit_final$fitted.values, fit_final$residuals), col = "blue")

# Squared residuals vs Fitted
plot(fit_final$fitted.values, fit_final$residuals^2,
     xlab = "Fitted values", ylab = "Squared Residuals",
     main = "Squared Residuals vs Fitted")
lines(lowess(fit_final$fitted.values, fit_final$residuals^2), col = "blue")

# QQ-plot
qqnorm(fit_final$residuals, main = "Normal Q-Q Plot (Final Model)")
qqline(fit_final$residuals, col = "red")

# Studentized residuals
stud_res_final <- rstudent(fit_final)
plot(fit_final$fitted.values, stud_res_final,
     xlab = "Fitted values", ylab = "Studentized Residuals",
     main = "Studentized Residuals vs Fitted")
abline(h = 0, col = "red", lty = 2)
lines(lowess(fit_final$fitted.values, stud_res_final), col = "blue")

```


```{r outliers vs population2}
par(mfrow = c(1, 2))
# Residuals vs population
plot(train_data$logpopulation, fit_final$residuals,
     xlab = "log(population)", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
lines(lowess(train_data$logpopulation, fit_final$residuals), col = "blue")

plot(train_data$logpopulation, stud_res_final,
     xlab = "log(population)", ylab = "Studentized Residuals")
abline(h = 0, col = "red", lty = 2)
lines(lowess(train_data$logpopulation, stud_res_final), col = "blue")

par(mfrow = c(1, 1))
```

## Model diagnostics

De plotjes voor dit gedeelte beperken tot de nuttige en die ook bespreken. hierin kan je goed zien dat er toch redelijk wat influential outliers zijn, dus dat robust wel is.

```{r}
library(olsrr)
olsrr::ols_plot_cooksd_bar(fit_final)
olsrr::ols_plot_resid_lev(fit_final, threshold = NULL, print_plot = TRUE)
olsrr::ols_plot_resid_stud_fit(fit_final, threshold = NULL, print_plot = TRUE)
```
```{r dfbetas plots, fig.width=10, fig.height=8}
car::dfbetasPlots(fit_final)
```
```{r dfbetas plots, fig.width=10, fig.height=8}
# IMPORTANT remark: the labelled observations are the points with 1,5 times the threshold (the reason is the visualisation)
# DFBETAS values (placing them in a data frame ensures easy compatibility with ggplot2)
dfbetas_vals <- data.frame(dfbetas(fit_final))

# Plot
colnames_dfbetas <- colnames(dfbetas_vals)
plots <- lapply(colnames_dfbetas[-1], function(predictor) {
ggplot(dfbetas_vals) +
  geom_point(aes(x = 1:nrow(dfbetas_vals), y = dfbetas_vals[[predictor]])) +
  geom_segment(aes(x = 1:nrow(dfbetas_vals), xend = 1:nrow(dfbetas_vals), y = 0, yend = dfbetas_vals[[predictor]]),
               color = 'cornflowerblue') +
  geom_hline(yintercept = c(2, -2) / sqrt(nrow(dfbetas_vals)), color = 'salmon') +
  labs(x = 'Observation index', y = 'DFBETAS',
       title = paste0('DFBETAS values for coefficient of ', predictor),
       subtitle = 'Thresholds are at \u00B1(2\u00F7\u221An)') +
  geom_text(aes(x = 1:nrow(dfbetas_vals), y = ifelse(dfbetas_vals[[predictor]] > 0, dfbetas_vals[[predictor]] + .05, dfbetas_vals[[predictor]] - .05),
                label = ifelse(abs(dfbetas_vals[[predictor]]) > 1.5*(2/sqrt(nrow(dfbetas_vals))),
                               paste0(round(dfbetas_vals[[predictor]], digits = 2), ' (', 1:nrow(dfbetas_vals), ')'), '')))
})
# Print 9 plots per pg
print(plots)
```

Diagnostic plots (figuur) reveal several potential outliers and high-leverage points. To assess the impact of these points, we fit a robust regression using Bisquare weights. 

```{r robust-inference}
library(MASS)

robust <- rlm(formula(fit_final), data = train_data, psi = psi.bisquare)
summary(robust, method = "XtX")
```

```{r}
par(mfrow=c(2,2))
plot(fit_final)
par(mfrow=c(2,2))
plot(robust, which = 1)
qqnorm(resid(robust))
qqline(resid(robust))

```

A comparison of the coefficients reveal no substantial differences, indicating that the influential points are not the result of gross errors. <hier moeten nog extra checks van influence on inferences bij>.
The identified leverage-points can be called 'good leverages' as they exhibit high leverage but their residuals are still small, this even improves the precision of the regression coefficients. (zie referentie) It'

```{r influence inference}
# Compare OLS vs Robust coefficients
comparison <- data.frame(
  OLS = coef(fit_final),
  Robust = coef(robust),
  Difference = coef(fit_final) - coef(robust)
)

print(round(comparison, 4))
```


### Outlier and Influence Diagnostics 

We use several diagnostic measures to identify influential observations

```{r influence diagnostics}
# Calculate diagnostics
stud_res_final <- rstudent(fit_final)
leverage <- hatvalues(fit_final)
p <- length(coef(fit_final))
n_train <- nrow(train_data)
leverage_threshold <- 2 * p / n_train
cooks_d <- cooks.distance(fit_final)
dffits_val <- dffits(fit_final)
dffits_threshold <- 2 * sqrt(p / n_train)
dfbetas_val <- dfbetas(fit_final)
dfbetas_threshold <- 2 / sqrt(n_train)

# dataframe
diagnostics <- data.frame(
  obs = 1:n_train,
  population = train_data$population,
  # zowel studentized deleted residuals als studentized residuals insteken?
  # DFBETAS ook insteken (per beta kan je dat bepalen) (chapter 10 Diagnostics slide 40)
  stud_residual = stud_res_final,
  leverage = leverage,
  cooks_d = cooks_d,
  dffits = dffits_val
)

# Flag observations
diagnostics$outlier_residual <- abs(diagnostics$stud_residual) > 2
diagnostics$high_leverage <- diagnostics$leverage > leverage_threshold
diagnostics$high_cooks <- diagnostics$cooks_d > 4 / n_train
diagnostics$high_dffits <- abs(diagnostics$dffits) > dffits_threshold

# summ
cat("Outliers by studentized residuals (|r*| > 2):", sum(diagnostics$outlier_residual), "\n")
cat("High leverage observations (h >", round(leverage_threshold, 4), "):", 
    sum(diagnostics$high_leverage), "\n")
cat("High Cook's distance (D >", round(4/n_train, 4), "):", 
    sum(diagnostics$high_cooks), "\n")
cat("High DFFITS (|DFFITS| >", round(dffits_threshold, 4), "):", 
    sum(diagnostics$high_dffits), "\n")
```

```{r influence plot, fig.width=8, fig.height=6}
# Influence plot
plot(leverage, stud_res_final, 
     xlab = "Leverage", ylab = "Studentized Residuals",
     main = "Influence Plot",
     cex = sqrt(cooks_d) * 10)
```


## interpretation final model

Our final model is:`r formula_final`.
```{r}
summary(fit_final)
print(confint(fit_final))
```

```{r}
library(stdmod)
b <- coef(fit_final)
#mean_race <- mean(train_data$racepctblack)
#mean_9thgrade <- mean(train_data$PctLess9thGrade)
mean_bsormore <- mean(train_data$PctBSorMore)
mean_poverty <- mean(train_data$PctPopUnderPov)

coef_mean <- b["PctPopUnderPov"] +
      #b["PctPopUnderPov:racepctblack"]    * mean_race +
      #b["PctPopUnderPov:PctLess9thGrade"] * mean_9thgrade +
      b["PctPopUnderPov:PctBSorMore"]     * mean_bsormore
percentage_change <- (exp(coef_mean) - 1) *100
confidence_intervals <- confint(fit_final)
```

#### Notes on interpretation

Our final model is a log-linear model, this makes interpretation of the coefficients less intuitive. We will start by giving the coefficients that express the estimated change in the log of the expected violent crime rate per 100k population with an increase of the predictor variable by 1 unit. To ease interpretation we will back transform this coefficient and multiply it by 100 to express the estimated percentage change of the mean violent crime rate / 100k when the predictor variable is increased by 1 unit. The intercept on its own is not interpretable as it would require all predictor variables (and interaction terms) to be zero. This is not in the scope of our model. If it were, the intercept of `r b["(intercept)"]` could be interpreted as the estimated log of expected crime rate when all predictor variables are 0.
There is no use in interpreting interaction-term coefficients on their own, as they indicate the effect of one predictor on the other, but have no direct effect on its own.


#### Interpretation of PctPopUnderPov

```{r}
set.seed(987654321)
ci_pov <- cond_effect_boot(fit_final,
            x = "PctPopUnderPov", w = "PctBSorMore", nboot = 2000)
ci_lower <- (exp(ci_pov$"CI Lower"[ci_pov$Level == "Medium"]) - 1) * 100 
ci_upper <- (exp(ci_pov$"CI Upper"[ci_pov$Level == "Medium"]) - 1) * 100
```

The effect of PctPopUnderPov varies with PctBsorMore through the interaction terms. This means that the effect of PctPopUnderPov is not simply given by its coefficient, but instead we can look at the estimated effect of PctPopUnderPov on the log of expected violent crimes as `r b["PctPopUnderPov"]` + `r b["PctPopUnderPov:PctBSorMore]` * PctBSorMore.  Interpretation is more intuitive if we back transform the log transformed data. The effect of an increase in PctPopUnderPop with one unit (one percent) on violent crimes per 100k is then calculated by:  exp(`r b["PctPopUnderPov"]` + `r b["PctPopUnderPov:PctBSorMore"]` * PctBSorMore) -1 when all other variables are held constant. 
If we take for example the mean of the PctBSorMore to calculate the effect of PctPoponderpov when the interacting variable is held at its average and we back transform it we can find `r percentage_change`. The confidence interval of this effect is [`r ci_lower`%, `r ci_higher`%]
With a confidence coefficient of .95 we estimate that the true percentage change in expected value of violent crimes (per population of 100k) per percentage increase of pop under pov is somewhere between `r ci_lower`% and `r ci_higher`%, when the interacting variable is held at its average and all other variables are held constant.  The interaction effect is reinforcing: with increasing percentages of people with higher education, the effect of PctPopUnderPov on violent crimes gets larger. Theoretically this could be explained due to the bigger gap in socio-economic standing, which could provoke more violent crimes.


#### Interpretation of PctBSorMore

```{r}
set.seed(987654321)
ci_pov2 <- cond_effect_boot(fit_final,
            x = "PctBSorMore", w = "PctPopUnderPov", nboot = 2000)
ci_lower2 <- (exp(ci_pov2$"CI Lower"[ci_pov2$Level == "Medium"]) - 1) * 100 
ci_upper2 <- (exp(ci_pov2$"CI Upper"[ci_pov2$Level == "Medium"]) - 1) * 100
```

The interpretation for PctBSorMore is similar to the one above because of their interaction. The conditional effect on of PctBSorMore on the log expected violent crime rate is given by: `r b["PctBSorMore"]` + `r b["PctPopUnderPov:PctBSorMore"]`* PctPopUnderPov.  Similarly to before we can take the mean of PctPopUnderPov and estimate the conditional effect of PctBSorMore on the log expected value of crime rate per unit increase of PctBSorMore, when keeping poverty at its mean and all other variables constant: `r b["PctBSorMore"]` + `r b["PctPopUnderPov:PctBSorMore"] * mean_poverty`. Contrary to above, we now see a interference effect: for higher levels of PctPopUnderPov the negative effect of PctBSorMore becomes less negative (the slope becomes less steep). Undoing the log transformation we get `r (exp(b["PctBSorMore"] + b["PctPopUnderPov:PctBSorMore"] * mean_poverty) - 1)* 100`%, with a confidence interval of [`r ci_lower2`%, `r ci_higher`%] which can be interpret as: with a confidence coefficient of 0.95 we estimated that the true percentage change in expected crime rate by increasing PctBSorMore by one unit (1%), keeping PctPovUnderPop at its mean and all other variables constant is somewhere between [`r ci_lower2`%, `r ci_higher`%]. Thus, under the conditions stated above, if a higher percentage of the population has a Bachelors degree or higher, the estimated mean crime rate decreases and this effect is less pronounced for higher levels of PctUnderPov.

#### Interpretation of race_black_logit  

The interpretation of race_black_logit is more complicated as on top of the log transformed response variable, we now also have a logit transformed predictor variable. It has no interaction terms so we can interpret the main effect on its own. A one-unit increase in the log-odds of percentage African American is associated with a `r b["race_black_logit"]` change in the log of Violent Crimes per 100k when keeping all other variables constant; where a one-unit increase in the log-odds has to be interpreted as a multiplication of the odds of being African American by e ($\approx$ 2.72). The exponentiated coefficient and 95% confidence interval are `r exp(b["race_black_logit] -1) * 100`% and [`r exp(confidence_intervals[4,1] - 1) * 100`%, `r exp(confidence_intervals[4,2] - 1) * 100`%] respectively. With a confidence coefficient of 0.95 we estimated that the true percentage change in expected crime rate by increasing race_black_logit by one unit, keeping all other variables constant is somewhere between [`r exp(confidence_intervals[4,1] - 1) * 100`%, `r exp(confidence_intervals[4,2] - 1) * 100`%]. This is a positive association: for increasing levels of the log-odds of percentage African Americans, there is an increase in the violent crime rates per 100k. 


#### Interpretation of PctImmig -- nog niet af

The effect of PctImmig on log Violent crime rates is: `r b["PctImmig"]`. Exponentiating and multiplying  this coefficients by 100 we get: `r (exp(b["PctImmig"]) - 1)* 100`. The exponentiated 95% confidence interval is [`r exp(confidence_intervals[6,1]) - 1` , `r exp(confidence_intervals[6,2]) -1 `]. With confidence coefficient .95 we estimate that the percentage change in expected crime rates per 100k population per unit increase in PctImmig when keeping all other variables constant will be somewhere between `r (exp(confidence_intervals[6,1]) - 1) * 100`% and `r (exp(confidence_intervals[6,2]) -1) * 100`%. This increase in violent crimes associated with the increase in percentage of foreign borns agrees with the ....<????>


#### agePct12t29 -- nog niet af

For the percentage of people between the age of 12 and 29, we find a negative association with log of violent crime rates per 100k. This is expressed by a coefficient of `r b[agePct12t29]` with 95% confidence interval [`r confidence_intervals[5,1]`, `r confidence_intervals[5,2]`]. Back transforming this confidence interval allows us to interpret it as: with 95% confidence we estimate that the true percentage change in expected crime rates per 100k population per unit increase in agepct12t29 (one percent) is somewhere between `r exp(confidence_intervals[5,1] -1) * 100`% and `r exp(confidence_intervals[5,2] - 1) * 100`%, when all other variables are held constant. This negative association is not in line with our prior assumption.

#### Family confidence interval



## Summary
Dusja multivariate model stuk beter dan univariate model als je kijkt naar de tabel

```{r summary}
# summary

mse_final <- mean(fit_final$residuals^2)
summary_results <- data.frame(
  Model = c("Simple (PctPopUnderPov only)", "Final Multivariate"),
  R_squared = c(round(summary(fit_simple)$r.squared, 4),
                      round(summary(fit_final)$r.squared, 4)),
  Adj_R_squared = c(round(summary(fit_simple)$adj.r.squared, 4),
                          round(summary(fit_final)$adj.r.squared, 4)),
  MSE = c(round(mse_simple, 2), round(mse_final, 2))
)

kable(summary_results, caption = "Comparison of Simple and Final Multivariate Models")
```

## model validation
Using the holdout set, we compute the Mean Squared Prediction Error (MSPR) and comparing it to the Mean Squared Error (MSPR) when the model is used to predict data in the training set.


```{r model validation backtransformation}
# Predictions on test set
# Transforming outcome variable in test dataset
test_data$logViolent <- log(test_data$ViolentCrimesPerPop + 1)
test_data$race_black_logit <- pct_to_logit(test_data$racepctblack)

# we backtransform the data to the normal scale
backtransformation <- function(value){
  exp(value) - 1
}
pred_interval_test_log <- predict(fit_final, newdata = test_data, interval = "prediction", level = 0.95)
pred_interval_train_log <- predict(fit_final, newdata = train_data, interval = "prediction", level = 0.95)
pred_interval_test <- apply(pred_interval_test_log, 1:2, backtransformation)
pred_interval_train <- apply(pred_interval_train_log, 1:2, backtransformation)
pred_interval_test_simple <- predict(fit_simple, newdata = test_data, interval = "prediction", level = 0.95)
pred_interval_train_simple <- predict(fit_simple, newdata = train_data, interval = "prediction", level = 0.95)

#mspr_training_log <- sum((train_data$logViolent - pred_interval_train_log[, "fit"])^2)/(n_training - 9)
```

As expected, 95% of the data points fall within the 95% prediction intervals. However, the prediction intervals are far too wide, making the predictions difficult to use.
```{r model validation prediction interval}
inside <- test_data$ViolentCrimesPerPop >= pred_interval_test[, "lwr"] & test_data$ViolentCrimesPerPop <= pred_interval_test[, "upr"]
cat("Fraction of datapoints in prediction interval:", mean(inside), "\n")

hist(pred_interval_test[, "upr"] - pred_interval_test[, "lwr"],
     main = "Width prediction interval",
     xlab = "Width prediction interval",
     xlim = c(0, 15000),
     breaks = 50)
```
```{r model plots}
# prediction intervals test data
df_prediction_test <- data.frame(observed = test_data$ViolentCrimesPerPop, predicted = pred_interval_test[, "fit"])

ggplot(df_prediction_test, aes(observed, predicted))+
  geom_point()+
  geom_smooth(se = FALSE)+
  geom_abline(intercept = 0, slope = 1, color = "red")+
  theme_bw() +
  labs(x = "number of violent crimes per 100K population (observed)",
       y = "number of violent crimes per 100K population (predicted)",
       title = "Test dataset")

# prediction intervals training data
df_prediction_train <- data.frame(observed = train_data$ViolentCrimesPerPop, predicted = pred_interval_train[, "fit"])

ggplot(df_prediction_train, aes(observed, predicted))+
  geom_point()+
  geom_smooth(se = FALSE)+
  geom_abline(intercept = 0, slope = 1, color = "red")+
  theme_bw() +
  labs(x = "number of violent crimes per 100K population (observed)",
       y = "number of violent crimes per 100K population (predicted)",
       title = "Train dataset")

# prediction intervals test data logscale
df_prediction_test_logscale <- data.frame(observed = test_data$logViolent, predicted = pred_interval_test_log[, "fit"])

ggplot(df_prediction_test_logscale, aes(observed, predicted))+
  geom_point()+
  geom_smooth(se = FALSE)+
  geom_abline(intercept = 0, slope = 1, color = "red")+
  theme_bw() +
  labs(x = "number of violent crimes per 100K population (observed)",
       y = "number of violent crimes per 100K population (predicted)",
       title = "Test dataset (log scale)")

# prediction intervals training data logscale
df_prediction_train_logscale <- data.frame(observed = train_data$logViolent, predicted = pred_interval_train_log[, "fit"])

ggplot(df_prediction_train_logscale, aes(observed, predicted))+
  geom_point()+
  geom_smooth(se = FALSE)+
  geom_abline(intercept = 0, slope = 1, color = "red")+
  theme_bw() +
  labs(x = "number of violent crimes per 100K population (observed)",
       y = "number of violent crimes per 100K population (predicted)",
       title = "Train dataset (log scale)")

# prediction intervals test data simple
df_prediction_test_simple <- data.frame(observed = test_data$ViolentCrimesPerPop, predicted = pred_interval_test_simple[, "fit"])

ggplot(df_prediction_test_simple, aes(observed, predicted))+
  geom_point()+
  geom_smooth(se = FALSE)+
  geom_abline(intercept = 0, slope = 1, color = "red")+
  theme_bw() +
  labs(x = "number of violent crimes per 100K population (observed)",
       y = "number of violent crimes per 100K population (predicted)",
       title = "Test dataset (univariate)")

# prediction intervals training data simple
df_prediction_train_simple <- data.frame(observed = train_data$ViolentCrimesPerPop, predicted = pred_interval_train_simple[, "fit"])

ggplot(df_prediction_train_simple, aes(observed, predicted))+
  geom_point()+
  geom_smooth(se = FALSE)+
  geom_abline(intercept = 0, slope = 1, color = "red")+
  theme_bw() +
  labs(x = "number of violent crimes per 100K population (observed)",
       y = "number of violent crimes per 100K population (predicted)",
       title = "Train dataset (univariate)")
```

```{r model validation MSPR and Rsquared, fig.width=8, fig.height=6}
# MSPR for test data
#mspr_test <- mean((test_data$ViolentCrimesPerPop - pred_interval_test[, "fit"])^2)
mspr_test <- mean((test_data$logViolent - pred_interval_test_log[, "fit"])^2)

# MSPR for training data
# mspr_train <- mean((train_data$ViolentCrimesPerPop - pred_interval_train[, "fit"])^2)
mspr_train <- mean((train_data$logViolent - pred_interval_train_log[, "fit"])^2)

cat("MSPR (training set):", round(mspr_train, 2), "\n")
cat("MSPR (test set):", round(mspr_test, 2), "\n")
cat("Ratio MSPR/MSE:", round(mspr_test/mspr_train, 4), "\n")

# R-squared on test data
#ss_res_test <- sum((test_data$ViolentCrimesPerPop - pred_interval_test[, "fit"])^2)
#ss_tot_test <- sum((test_data$ViolentCrimesPerPop - mean(test_data$ViolentCrimesPerPop))^2)
ss_res_test <- sum((test_data$logViolent - pred_interval_test_log[, "fit"])^2)
ss_tot_test <- sum((test_data$logViolent - mean(test_data$logViolent))^2)
r2_test <- 1 - ss_res_test/ss_tot_test
cat("R² on test set:", round(r2_test, 4), "\n")

# R-squared on training data
#ss_res_train <- sum((train_data$ViolentCrimesPerPop - pred_interval_train[, "fit"])^2)
#ss_tot_train <- sum((train_data$ViolentCrimesPerPop - mean(train_data$ViolentCrimesPerPop))^2)
ss_res_train <- sum((train_data$logViolent - pred_interval_train_log[, "fit"])^2)
ss_tot_train <- sum((train_data$logViolent - mean(train_data$logViolent))^2)
r2_train <- 1 - ss_res_train/ss_tot_train
cat("R² on training set:", round(r2_train, 4), "\n") 
```
```{r model validation MSPR and Rsquared backtransformed, fig.width=8, fig.height=6}
# MSPR for test data
mspr_test <- mean((test_data$ViolentCrimesPerPop - pred_interval_test[, "fit"])^2)

# MSPR for training data
mspr_train <- mean((train_data$ViolentCrimesPerPop - pred_interval_train[, "fit"])^2)

cat("MSPR (training set):", round(mspr_train, 2), "\n")
cat("MSPR (test set):", round(mspr_test, 2), "\n")
cat("Ratio MSPR/MSE:", round(mspr_test/mspr_train, 4), "\n")

#R-squared on test data
ss_res_test <- sum((test_data$ViolentCrimesPerPop - pred_interval_test[, "fit"])^2)
ss_tot_test <- sum((test_data$ViolentCrimesPerPop - mean(test_data$ViolentCrimesPerPop))^2)
r2_test <- 1 - ss_res_test/ss_tot_test
cat("R² on test set:", round(r2_test, 4), "\n")

# R-squared on training data
ss_res_train <- sum((train_data$ViolentCrimesPerPop - pred_interval_train[, "fit"])^2)
ss_tot_train <- sum((train_data$ViolentCrimesPerPop - mean(train_data$ViolentCrimesPerPop))^2)
r2_train <- 1 - ss_res_train/ss_tot_train
cat("R² on training set:", round(r2_train, 4), "\n") 

```

```{r model validation MSPR and Rsquared simple, fig.width=8, fig.height=6}
# MSPR for test data
mspr_test_simple <- mean((test_data$ViolentCrimesPerPop - pred_interval_test_simple[, "fit"])^2)

# MSPR for training data
mspr_train_simple <- mean((train_data$ViolentCrimesPerPop - pred_interval_train_simple[, "fit"])^2)

cat("MSPR (training set):", round(mspr_train_simple, 2), "\n")
cat("MSPR (test set):", round(mspr_test_simple, 2), "\n")
cat("Ratio MSPR/MSE:", round(mspr_test_simple/mspr_train_simple, 4), "\n")

# R-squared on test data
ss_res_test_simple <- sum((test_data$ViolentCrimesPerPop - pred_interval_test_simple[, "fit"])^2)
ss_tot_test <- sum((test_data$ViolentCrimesPerPop - mean(test_data$ViolentCrimesPerPop))^2)
r2_test_simple <- 1 - ss_res_test_simple/ss_tot_test
cat("R² on test set:", round(r2_test_simple, 4), "\n")

# R-squared on training data
ss_res_train_simple <- sum((train_data$ViolentCrimesPerPop - pred_interval_train_simple[, "fit"])^2)
ss_tot_train <- sum((train_data$ViolentCrimesPerPop - mean(train_data$ViolentCrimesPerPop))^2)
r2_train_simple <- 1 - ss_res_train_simple/ss_tot_train
cat("R² on training set:", round(r2_train_simple, 4), "\n") 
```


Resultaten zijn niet zo goed, dus dit nog niet herwerkt.
```{r model validation and population size}
logpop_values <- seq(4.1, 6, 0.1)

mspr_fun <- function(logpop, data, fit_final){
  # filtering
  data_filtered <- data[data$logpopulation < logpop + 0.1 & data$logpopulation > logpop - 0.1, ]
  # Predictions on test set
  pred <- exp(predict(fit_final, newdata = data_filtered))
  mspr <- mean((data_filtered$ViolentCrimesPerPop - pred)^2)
}

mspr_values_test <- sapply(logpop_values, FUN=mspr_fun, data=test_data, fit_final=fit_final)
mspr_values_train <- sapply(logpop_values, FUN=mspr_fun, data=train_data, fit_final=fit_final)

df_plot_logpop_mspr <- data.frame(logpop = logpop_values, mspr_values_test, mspr_values_train)

ggplot(df_plot_logpop_mspr, aes_string(logpop_values,mspr_values_test)) +
    geom_point(alpha = 0.6, size = 1.7) +
    geom_smooth(method = "loess", color = "red", se = FALSE)+
    theme_bw(base_size = 8) +
    labs(title = "Mean squared prediction error for test data vs logpop", x = "logpop", y = "mspr")
ggplot(df_plot_logpop_mspr, aes_string(logpop_values,mspr_values_train)) +
    geom_point(alpha = 0.6, size = 1.7) +
    geom_smooth(method = "loess", color = "red", se = FALSE)+
    theme_bw(base_size = 8) +
    labs(title = "Mean squared prediction error for training data vs logpop", x = "logpop", y = "mspr")
```


```{r}
fit_final <- lm(formula_final, data = test_data)
final_sums <- summary(fit_final)
final_sums
plot(fit_final)
```


```{r}
crimes_table_subset$logViolent <- log(crimes_table_subset$ViolentCrimesPerPop + 1)
crimes_table_subset$race_black_logit <- pct_to_logit(crimes_table_subset$racepctblack)

fit_final <- lm(formula_final, data = crimes_table_subset)
final_sums <- summary(fit_final)
final_sums
plot(fit_final)
```



# Statistical discussion

There are several characteristics of the dataset that require discussion. First, we focus on how the missing data in our dataset affects our found results. The main source of missing data in our analysis, is the ViolentCrimesPerPop variable, where 221 observations are missing because of incomplete rape reporting in Midwestern States. This selective missingness could bias estimates if these communities systematically differ in crime levels or demographic characteristics. Therefore, in the descriptives section, we tested whether the characteristics of the communities with missing data differed from those of the non-missing data, and we did not find any clear differences. We thus assume that this missingness does not have an important effect on our found results. However, if these communities are different in ways not captured by our measured variables, bias could remain.

Second, the data from the LEMAS survey includes all communities with police departments with at least 100 officers, but only a random sample of smaller ones. Because of this, large (and thus more urban) communities are overrepresented, while small communities are underrepresented in the dataset. Thus, the relationships we find in the analysis are more likely to hold in the context of larger cities, and may be less likely to translate to rural communities.

Additionally, based on our dataset, we expected that smaller communities have greater variance in both predictor and outcome variables, which we see confirmed in our scatter plots. The reason for this is that per capita crime rates in small populations are more volatile, as one crime in a community of 1,000 translates to 100 per 100,000 population, making small fluctuations in actual crime numbers translate to large fluctuations in rates. We dealt with this heteroskedasticity by applying a log transformation. Nevertheless, variability for these smaller communities should thus be interpreted with caution.

Another caveat is that the FBI’s data is from 1995, while the Census populations containing demographic variables are from 1990. This implies that data from 1990 is used to predict crime rates from 1995.Thus, for communities that experienced large demographic changes in this timespan, the predictor variables would not accurately represent the communities at the times when the crimes occurred. If, for example, the poverty rate would increase substantially between 1990 and 1995, the outdated 1990 poverty rate in our analysis might underestimate the true relationship. However, we expect that, because of the short timespan and as we expect most of the socio-economic variables we used to change only slowly, this to have a limited impact on our conclusions.

At last, the data are from 1990-1995. Since then, the social and economic environment have changed, implying that our estimated relationships would not hold today as crime rates, reporting practices,... have evolved. Thus, these results should be carefully intepreted in today's context as the mechanisms driving these relationships are different in comparison to when the data collection happened.  

We conclude that our regression analysis results are thus valid for a subset of U.S. communities in the 1990's and that the results should be cautiously interpreted in this context.

# References

Becker GS (1968) Crime and Punishment: An Economic Approach. Journal of Political Economy 76: 169–217

# References dataset

U. S. Department of Commerce, Bureau of the Census, Census Of Population And Housing 
1990 United States: Summary Tape File 1a & 3a (Computer Files),

U.S. Department Of Commerce, Bureau Of The Census Producer, Washington, DC and 
Inter-university Consortium for Political and Social Research Ann Arbor, Michigan. 
(1992)

U.S. Department of Justice, Bureau of Justice Statistics, Law Enforcement Management 
And Administrative Statistics (Computer File) U.S. Department Of Commerce, Bureau Of 
The Census Producer, Washington, DC and Inter-university Consortium for Political and 
Social Research Ann Arbor, Michigan. (1992)

U.S. Department of Justice, Federal Bureau of Investigation, Crime in the United 
States (Computer File) (1995)

# Additional references

Redmond, M. A. and A. Baveja: A Data-Driven Software Tool for Enabling Cooperative 
Information Sharing Among Police Departments. European Journal of Operational Research 
141 (2002) 660-678.

Rousseeuw, P. J., & van Zomeren, B. C. (1990). Unmasking Multivariate Outliers and Leverage Points. Journal of the American Statistical Association, 85(411), 633–639. https://doi.org/10.1080/01621459.1990.10474920

# Source - https://stackoverflow.com/a
# Posted by DonJ, modified by community. See post 'Timeline' for change history
# Retrieved 2025-12-03, License - CC BY-SA 3.0

```{r show-code, ref.label=all_labels(), echo = TRUE, eval=FALSE}

```



